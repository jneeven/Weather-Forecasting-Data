{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binarized Neural Networks cheatsheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jneeven/Weather-Forecasting-Data/blob/master/Binarized_Neural_Networks_cheatsheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayU21JV8j5Z3",
        "colab_type": "text"
      },
      "source": [
        "## Quantizers and Gradients\n",
        "Both explained at\n",
        "https://larq.dev/api/quantizers/\n",
        "\n",
        "\n",
        "#### Straight-through Estimator\n",
        "\n",
        "Straight-through estimator uses sign function for activation:\n",
        "\n",
        "`f(x) = -1 if x < 0 else 1`\n",
        "\n",
        "and the following gradient function:\n",
        "\n",
        "`f'(x) = 1 if abs(x) <= 1 else 0`\n",
        "\n",
        "<br/>\n",
        "\n",
        "Not sure what the reasoning behind this is: if the activation is small, gradient is 1, and if it is large, gradient is 0. \\\n",
        "Is explained in [Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or âˆ’1](https://arxiv.org/pdf/1602.02830.pdf).\\\n",
        "According to that, *the gradient is canceled if x is too large*, and not doing this significantly worsens performance.\n",
        "\n",
        "\n",
        "\n",
        "## Shift-based BatchNorm\n",
        "For efficiency, we need some kind of batchnorm that doesn't require calculating the running mean and standard deviation, and then dividing by it, as these are all expensive operations. We therefore use a kind of batchnorm based on bit shifts.\n",
        "\n",
        "If x is the vector of activations of a given layer for one batch, we get\n",
        "````\n",
        "def SBN(x, gamma, epsilon=1e-8):\n",
        "    batch_mean = np.mean(x)\n",
        "    centered = x - batch_mean\n",
        "    approx_variance = np.mean(centered * << >> AP2(centered))\n",
        "    normalized = centered << >> AP2(1 / np.sqrt(approx_variance + epsilon))\n",
        "    denormalized = AP2(gamma) << >> normalized\n",
        "  \n",
        "````\n",
        "Where gamma is a learnable parameter, and AP2 is the approximate power of 2:\n",
        "````\n",
        "def AP2(x):\n",
        "    return sign(x) * \n",
        "````\n",
        "\n",
        "## MISC\n",
        "Bengio BNN paper also clips the real-valued weights to -1 and 1, because they'd otherwise grow very large without any impact on the binarized weights.\n"
      ]
    }
  ]
}